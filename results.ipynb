{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca53ad2-ce83-42c4-80b5-2b20a6ae9e27",
   "metadata": {},
   "source": [
    "# Notebook for generating the images for the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3be1091-96ca-4c82-9496-783138602678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from architecture.Model import Model\n",
    "from dataloader.Dataloader import *\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from torchvision.transforms.functional import resize\n",
    "from munch import Munch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f36404-2ea7-460a-afd5-0bc801491d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_imgs(tensor):\n",
    "    X = torch.permute(tensor, [0,2,3,1]).cpu().detach().numpy()\n",
    "    x_n = [(x-x.min())/(x.max()-x.min()) for x in X]\n",
    "    imgs = [img for img in x_n]\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0dad81-b72b-42c7-8ad4-51c9447fff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_figure(ref_imgs, src_imgs, fake_imgs_list):\n",
    "    plt.figure(figsize=(15,2), dpi=400)\n",
    "    plt.subplot(1,len(src_imgs)+1,1)\n",
    "    plt.imshow(np.ones(ref_imgs[0].shape))\n",
    "    plt.axis('off')\n",
    "    plt.suptitle(\"Source Images\")\n",
    "    for i, src_img in enumerate(src_imgs):\n",
    "        plt.subplot(1,len(src_imgs)+1,i+2)\n",
    "        plt.imshow(src_img)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    for i,(ref_img, fake_imgs) in enumerate(zip(ref_imgs, fake_imgs_list)):\n",
    "        plt.figure(figsize=(15,3), dpi=400)\n",
    "        plt.subplot(1,len(fake_imgs)+1,1)\n",
    "        plt.imshow(ref_img)\n",
    "        plt.axis('off')\n",
    "        if i == 0 :\n",
    "            plt.title(\"Reference Image\")\n",
    "        #plt.suptitle(\"Generated Images\")\n",
    "        for i, fake_img in enumerate(fake_imgs):\n",
    "            plt.subplot(1,len(fake_imgs)+1,i+2)\n",
    "            plt.imshow(fake_img)\n",
    "            plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c3cc4-e06f-44d3-9814-92d83f30d545",
   "metadata": {},
   "source": [
    "## CELEBA_HQ\n",
    "Vizualisation of reference and latent guided generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5b1e70-d2e0-4862-a23c-97d04d8f860f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator\n",
      "mapping_network\n",
      "style_encoder\n",
      "discriminator\n",
      "fan\n"
     ]
    }
   ],
   "source": [
    "#import celeba pretrained model\n",
    "model_params = Munch({\n",
    "    'img_size': 256, #256\n",
    "    'latent_dim': 16,\n",
    "    'style_dim': 64,\n",
    "    'num_domains' : 2, #celeba\n",
    "    'fan_pretrained_fname' : \"architecture/FAN/weights.pth\",\n",
    "    \"wFilter\" : 1 # 0<= -> no FAN\n",
    "})\n",
    "\n",
    "nets, _ = Model(model_params) #instantiate model\n",
    "cpt = torch.load(\"runs/celeba_model/000010__networs_copy.cpt\") #take the copy weights -> moving average improves quality\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for model_name, model in nets.items():\n",
    "    print(model_name)\n",
    "    if model_name in cpt:\n",
    "        model.load_state_dict(cpt[model_name])\n",
    "        model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "086cb067-fb52-41b1-b5dc-a63f00fdd8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load male and female datasets\n",
    "root = \"../shared/stargan_folder/data/celeba_hq/val\"\n",
    "batch_size=6\n",
    "\n",
    "#contains both male and female imgs. Returns corresponding label.\n",
    "test_loader = get_loader(root, batch_size, model_params.img_size, chunk=\"test\")\n",
    "test_fetcher = Fetcher(test_loader,chunk=\"test\")\n",
    "\n",
    "male_path=os.path.join(root,\"male\")\n",
    "male_loader = get_loader(male_path, batch_size//2, model_params.img_size, chunk=\"eval\")\n",
    "male_fetcher = Fetcher(male_loader, chunk=\"eval\")\n",
    "\n",
    "female_path=os.path.join(root,\"female\")\n",
    "female_loader = get_loader(female_path, batch_size//2, model_params.img_size, chunk=\"eval\")\n",
    "female_fetcher = Fetcher(female_loader, chunk=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25f2037c-10c5-4a73-a3e8-5c46b4fe06f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source \n",
    "src_inputs = next(test_fetcher)\n",
    "x_src, y_src = src_inputs.x, src_inputs.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "569e1180-d18c-4c4c-b304-5c06796d5119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random src to female trg (ref)\n",
    "#reference = target\n",
    "x_ref_female = next(female_fetcher)\n",
    "y_ref = torch.tensor([0]*len(x_ref_female)).to(device)\n",
    "\n",
    "#generate style from trg domain\n",
    "style = nets.style_encoder(x_ref_female, y_ref)\n",
    "#copy the styles to apply to each src input\n",
    "styles = (style.unsqueeze(1)).repeat(1,len(x_src),1) #add fake dimension at dim=1 and copy repeat every style N times to compatibility with src_dim\n",
    "\n",
    "#generate source masks from FAN\n",
    "masks=nets.fan.get_heatmap(x_src)\n",
    "\n",
    "#generate reference guided output for every source to each reference\n",
    "x_fakes_female=[] #list of outputs from every reference\n",
    "for style in styles:\n",
    "    x_fake = nets.generator(x_src, style, masks)\n",
    "    x_fakes_female.append(x_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb0ddd3b-16a2-4243-a261-49aafa935706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random src to male trg (ref)\n",
    "#reference = target\n",
    "x_ref_male = next(male_fetcher)\n",
    "y_ref = torch.tensor([1]*len(x_ref_male)).to(device)\n",
    "\n",
    "#generate style from trg domain\n",
    "style = nets.style_encoder(x_ref_male, y_ref)\n",
    "#copy the styles to apply to each src input\n",
    "styles = (style.unsqueeze(1)).repeat(1,len(x_src),1) #add fake dimension at dim=1 and copy repeat every style N times to compatibility with src_dim\n",
    "\n",
    "#generate source masks from FAN\n",
    "masks=nets.fan.get_heatmap(x_src)\n",
    "\n",
    "#generate reference guided output for every source to each reference\n",
    "x_fakes_male=[] #list of outputs from every reference\n",
    "for style in styles:\n",
    "    x_fake = nets.generator(x_src, style, masks)\n",
    "    x_fakes_male.append(x_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "160a1f78-798f-477f-bab0-b062766c45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_imgs_male = tensor_to_imgs(x_ref_male)\n",
    "ref_imgs_female = tensor_to_imgs(x_ref_female)\n",
    "src_imgs = tensor_to_imgs(x_src)\n",
    "fake_imgs_list = {\"male\" : [tensor_to_imgs(tensor) for tensor in x_fakes_male],\n",
    "                  \"female\" : [tensor_to_imgs(tensor) for tensor in x_fakes_female]}#len(ref)xlen(src) -> 3x6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "090b4b12-d38c-4408-b49f-000c70c59cdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'ref_img' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_imgs_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mgenerate_figure\u001b[0;34m(ref_imgs, src_imgs, fake_imgs_list)\u001b[0m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(src_imgs)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39mones(\u001b[43mref_img\u001b[49m\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39msuptitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource Images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'ref_img' referenced before assignment"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_figure(ref_imgs_male, src_imgs, fake_imgs_list['male'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61425b49-ffbb-4eda-89c6-e750ffd0b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_figure(ref_imgs_female, src_imgs, fake_imgs_list['female'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd673ba7",
   "metadata": {},
   "source": [
    "Latent guided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518d24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source \n",
    "src_inputs = next(test_fetcher)\n",
    "x_src, y_src = src_inputs.x, src_inputs.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc6f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate style from latent vector and trg domain\n",
    "z = torch.randn(len(x_src),model_params.latent_dim).to(device)\n",
    "y_trg = torch.logical_not(y_src).int() #trg domain chosen as opposite from src domain\n",
    "\n",
    "style = nets.mapping_network(z,y_trg)\n",
    "\n",
    "#generate source masks from FAN\n",
    "masks=nets.fan.get_heatmap(x_src)\n",
    "\n",
    "x_fake = nets.generator(x_src, style, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_imgs = tensor_to_imgs(x_src)\n",
    "fake_imgs = tensor_to_imgs(x_fake) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2ffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,6),dpi=400)\n",
    "for i, (src_img, fake_img) in enumerate(zip(src_imgs, fake_imgs)):\n",
    "\n",
    "    plt.subplot(len(src_imgs),2,2*i+1)\n",
    "    if i==0 : plt.title(\"Source image\")\n",
    "    plt.imshow(src_img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(len(src_imgs),2,2*i+2)\n",
    "    if i==0 : plt.title(\"Generated image\")\n",
    "    plt.imshow(fake_img)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093ba7ee-9f7a-408a-b161-be44336591e7",
   "metadata": {},
   "source": [
    "## AFHQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f53d2cc-a638-4d39-94b5-60da45e4b15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator\n",
      "mapping_network\n",
      "style_encoder\n",
      "discriminator\n"
     ]
    }
   ],
   "source": [
    "#import celeba pretrained model\n",
    "model_params = Munch({\n",
    "    'img_size': 256, #256\n",
    "    'latent_dim': 16,\n",
    "    'style_dim': 64,\n",
    "    'num_domains' : 3, #afhq\n",
    "    'fan_pretrained_fname' : \"architecture/FAN/weights.pth\",\n",
    "    \"wFilter\" : 0 # 0<= -> no FAN\n",
    "})\n",
    "\n",
    "nets, _ = Model(model_params) #instantiate model\n",
    "cpt = torch.load(\"runs/afhq_model/000010__networs_copy.cpt\") #take the copy weights -> moving average improves quality\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for model_name, model in nets.items():\n",
    "    print(model_name)\n",
    "    if model_name in cpt:\n",
    "        model.load_state_dict(cpt[model_name])\n",
    "        model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaeea4a6-1de6-40eb-bb11-d7b860d6da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dog, cat and wild datasets\n",
    "root = \"../shared/stargan_folder/data/afhq/val\"\n",
    "batch_size=6\n",
    "\n",
    "#contains all domains' imgs. Returns corresponding label.\n",
    "test_loader = get_loader(root, batch_size, model_params.img_size, chunk=\"test\")\n",
    "test_fetcher = Fetcher(test_loader,chunk=\"test\")\n",
    "\n",
    "dog_path=os.path.join(root,\"dog\")\n",
    "dog_loader = get_loader(dog_path, batch_size//2, model_params.img_size, chunk=\"eval\")\n",
    "dog_fetcher = Fetcher(dog_loader, chunk=\"eval\")\n",
    "\n",
    "cat_path=os.path.join(root,\"cat\")\n",
    "cat_loader = get_loader(cat_path, batch_size//2, model_params.img_size, chunk=\"eval\")\n",
    "cat_fetcher = Fetcher(cat_loader, chunk=\"eval\")\n",
    "\n",
    "wild_path=os.path.join(root,\"wild\")\n",
    "wild_loader = get_loader(wild_path, batch_size//2, model_params.img_size, chunk=\"eval\")\n",
    "wild_fetcher = Fetcher(wild_loader, chunk=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c299dc12-27b7-4b24-8b46-5d014314eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source \n",
    "src_inputs = next(test_fetcher)\n",
    "x_src, y_src = src_inputs.x, src_inputs.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c8f3f01-d403-4ccc-bcaf-80f2439e9928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random src to dog trg (ref)\n",
    "#reference = target\n",
    "x_ref_dog = next(dog_fetcher)\n",
    "y_ref = torch.tensor([0]*len(x_ref_dog)).to(device)\n",
    "\n",
    "#generate style from trg domain\n",
    "style = nets.style_encoder(x_ref_dog, y_ref)\n",
    "#copy the styles to apply to each src input\n",
    "styles = (style.unsqueeze(1)).repeat(1,len(x_src),1) #add fake dimension at dim=1 and copy repeat every style N times to compatibility with src_dim\n",
    "\n",
    "#generate reference guided output for every source to each reference\n",
    "x_fakes_dog=[] #list of outputs from every reference\n",
    "for style in styles:\n",
    "    x_fake = nets.generator(x_src, style, None)\n",
    "    x_fakes_dog.append(x_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2bf7773-3e13-4799-a600-7dfbb39b8ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random src to dog trg (ref)\n",
    "#reference = target\n",
    "x_ref_cat = next(cat_fetcher)\n",
    "y_ref = torch.tensor([0]*len(x_ref_cat)).to(device)\n",
    "\n",
    "#generate style from trg domain\n",
    "style = nets.style_encoder(x_ref_cat, y_ref)\n",
    "#copy the styles to apply to each src input\n",
    "styles = (style.unsqueeze(1)).repeat(1,len(x_src),1) #add fake dimension at dim=1 and copy repeat every style N times to compatibility with src_dim\n",
    "\n",
    "#generate reference guided output for every source to each reference\n",
    "x_fakes_cat=[] #list of outputs from every reference\n",
    "for style in styles:\n",
    "    x_fake = nets.generator(x_src, style, None)\n",
    "    x_fakes_cat.append(x_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3e8d0e3-2bbb-456a-ae0d-8d71f97756c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random src to dog trg (ref)\n",
    "#reference = target\n",
    "x_ref_wild = next(wild_fetcher)\n",
    "y_ref = torch.tensor([0]*len(x_ref_wild)).to(device)\n",
    "\n",
    "#generate style from trg domain\n",
    "style = nets.style_encoder(x_ref_wild, y_ref)\n",
    "#copy the styles to apply to each src input\n",
    "styles = (style.unsqueeze(1)).repeat(1,len(x_src),1) #add fake dimension at dim=1 and copy repeat every style N times to compatibility with src_dim\n",
    "\n",
    "#generate reference guided output for every source to each reference\n",
    "x_fakes_wild=[] #list of outputs from every reference\n",
    "for style in styles:\n",
    "    x_fake = nets.generator(x_src, style, None)\n",
    "    x_fakes_wild.append(x_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "304edf35-664e-4704-a185-e62f0c16e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_imgs_dog = tensor_to_imgs(x_ref_dog)\n",
    "ref_imgs_cat = tensor_to_imgs(x_ref_cat)\n",
    "ref_imgs_wild = tensor_to_imgs(x_ref_wild)\n",
    "src_imgs = tensor_to_imgs(x_src)\n",
    "fake_imgs_list = {\"dog\" : [tensor_to_imgs(tensor) for tensor in x_fakes_dog],\n",
    "                  \"cat\" : [tensor_to_imgs(tensor) for tensor in x_fakes_cat],\n",
    "                  \"wild\" : [tensor_to_imgs(tensor) for tensor in x_fakes_wild]}#len(ref)xlen(src) -> 3x6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3423c23-debb-4ce8-97d1-29802df00005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
